{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SQgYNX8hfFqP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%store -r X_tfidf_sparse\n",
        "%store -r X_tfidf_sparse_test\n",
        "%store -r y_test\n",
        "%store -r y_train_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegressionMulticlass:\n",
        "    def __init__(self, learning_rate=0.3, num_iterations=10000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "    \n",
        "    \n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "    \n",
        "    def xavier_init(self, shape):\n",
        "        fan_in = shape[0]\n",
        "        fan_out = shape[1]\n",
        "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "        return np.random.uniform(-limit, limit, size=shape)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "        self.weights = self.xavier_init((num_features, num_classes))\n",
        "        self.bias = np.zeros((1, num_classes))\n",
        "        y_one_hot = np.eye(num_classes)[y]\n",
        "        \n",
        "        for _ in range(self.num_iterations):\n",
        "            linear_model = X.dot(self.weights) + self.bias\n",
        "            y_pred = self.softmax(linear_model)\n",
        "            \n",
        "            dw = (1 / num_samples) * X.T.dot(y_pred - y_one_hot)\n",
        "            db = (1 / num_samples) * np.sum(y_pred - y_one_hot, axis=0, keepdims=True)\n",
        "            \n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "    \n",
        "            \n",
        "    def predict(self, X):\n",
        "\n",
        "        num_samples, num_features = X.shape\n",
        "        _, num_classes = self.weights.shape\n",
        "    \n",
        "        if num_features != self.weights.shape[0]:\n",
        "            self.weights = self.xavier_init((num_features, num_classes))\n",
        "            \n",
        "        linear_predictions = X.dot(self.weights) + self.bias\n",
        "        y_pred = self.softmax(linear_predictions)\n",
        "        print (self.weights.shape)\n",
        "        class_pred = np.argmax(y_pred, axis=1)  # Choose the class with the highest probability\n",
        "        return class_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_emotions = LogisticRegressionMulticlass()\n",
        "model_emotions.fit(X_tfidf_sparse, y_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7420, 7)\n",
            "Micro-average F1-score: 0.7586656727543793\n",
            "F1-score on training data: 0.7590430169509217\n"
          ]
        }
      ],
      "source": [
        "pred = model_emotions.predict (X_tfidf_sparse)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "micro_average_f1 = f1_score(pred, y_train_encoded, average='micro')\n",
        "print(\"Micro-average F1-score:\", micro_average_f1)\n",
        "\n",
        "f1_external = f1_score(pred, y_train_encoded, average='weighted')\n",
        "print(\"F1-score on training data:\", f1_external)\n",
        "\n",
        "#print (pred [0:300],y_train_encoded [0:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7420, 7)\n",
            "[4 4 4 ... 6 4 4]\n",
            "F1-score on external test data: 0.5838355255505735\n",
            "Micro-average F1-score: 0.5860869565217391\n"
          ]
        }
      ],
      "source": [
        "# Predict labels using the trained logistic regression model\n",
        "\n",
        "y_external_pred = model_emotions.predict(X_tfidf_sparse_test)\n",
        "\n",
        "print (y_test_encoded)\n",
        "\n",
        "f1_external = f1_score(y_test_encoded, y_external_pred, average='weighted')\n",
        "print(\"F1-score on external test data:\", f1_external)\n",
        "micro_average_f1 = f1_score(y_test_encoded, y_external_pred, average='micro')\n",
        "print(\"Micro-average F1-score:\", micro_average_f1)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
