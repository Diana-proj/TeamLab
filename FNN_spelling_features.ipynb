{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SQgYNX8hfFqP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5366\n"
          ]
        }
      ],
      "source": [
        "# Loading the training file\n",
        "file_path = '/Users/diana/Desktop/isear-train.xlsx'\n",
        "\n",
        "# Define custom headers\n",
        "custom_headers = ['Emotions', 'Text']\n",
        "df = pd.read_excel(file_path, skiprows=1, header=None, names=custom_headers)\n",
        "\n",
        "# Display the first few rows\n",
        "print(len(df['Text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label preprocessing\n",
        "\n",
        "label_encoding = {'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n",
        "y = df['Emotions'].values\n",
        "y_train_encoded = np.array([label_encoding[label] for label in y])\n",
        "y_train_tensor = torch.tensor (y_train_encoded)\n",
        "\n",
        "#print(\"Encoded labels:\", y, y_train_encoded[:20])\n",
        "#print(len(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom data preprocessing\n",
        "\n",
        "def tokenize(text):\n",
        "    translation_table = str.maketrans({c: f' {c} ' if not c.isalnum() else c for c in set(text)})  # creates translation\n",
        "    # table(dictionary) with the built-in function maketrans, set(text) makes an unordered collection of unique\n",
        "    # elements through set comprehension, a concise way to create sets\n",
        "    tokenized_text = text.translate(translation_table)  # uses translation table to add whitespace around special\n",
        "    # characters and punctuation\n",
        "    return tokenized_text.strip().lower().split()  # split-tokens split on space, lower-tokens made all lowercase,\n",
        "    # strip-and leading or trailing whitespaces are removed from string\n",
        "\n",
        "import string \n",
        "\n",
        "text = ''.join(df['Text'].astype(str))\n",
        "#print (text)\n",
        "\n",
        "tokenized_text = []\n",
        "for index, sentence in enumerate(df['Text']):\n",
        "    emotion_label = df['Emotions'][index]\n",
        "    sentence = tokenize(sentence)\n",
        "    tokenized_text.append (sentence)\n",
        "\n",
        "# Filtering out digits and punctuation\n",
        "filtered_tokens = []\n",
        "\n",
        "for tokens in tokenized_text:\n",
        "    clean_tokens = []\n",
        "    \n",
        "    for token in tokens:\n",
        "        if all(char in string.punctuation for char in token):\n",
        "            continue  # Skip punctuation tokens\n",
        "        if token.isdigit():\n",
        "            continue  # Skip digit tokens\n",
        "        else:\n",
        "            clean_tokens.append(token)  \n",
        "    \n",
        "    filtered_tokens.append(clean_tokens)\n",
        "\n",
        "tokenized_text = filtered_tokens\n",
        "#print (filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correct misspelled words\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "from collections import Counter\n",
        "\n",
        "# Initialize the spell checker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Flatten the list of lists \n",
        "just_tokens = []\n",
        "for sentence in filtered_tokens:\n",
        "    for token in sentence:\n",
        "        just_tokens.append (token)\n",
        "\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_counts = Counter(just_tokens)\n",
        "\n",
        "# Find tokens that appear only once \n",
        "uncommon_words = [token for token in token_counts if token_counts[token] == 1]\n",
        "\n",
        "#print (len (uncommon_words))\n",
        "\n",
        "# Check if the uncommen words were misspelled \n",
        "misspelled_words = spell.unknown(uncommon_words)\n",
        "#print (misspelled_words)\n",
        "\n",
        "# Create a dictionary of misspelled words and their corrections\n",
        "corrections = {word: spell.correction(word) for word in misspelled_words}\n",
        "#print(\"Misspelled words and corrections:\", corrections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply corrections to the text\n",
        "\n",
        "def correct_spelling(text, corrections):\n",
        "    corrected_text = []\n",
        "    for sentence in text: # Correct each token in the sentence\n",
        "        corrected_tokens = [corrections.get(token, token) if corrections.get(token, token) else token for token in sentence]  # Correct the tokens\n",
        "        corrected_sentence = ' '.join(corrected_tokens)  # Join the tokens back into a string (sentence)\n",
        "        corrected_text.append(corrected_sentence)\n",
        "    return corrected_text\n",
        "\n",
        "# Apply spell correction to each sublist of sentences in the list of lists\n",
        "corrected_texts = [correct_spelling([sentence], corrections) for sentence in filtered_tokens]\n",
        "#print (corrected_texts)\n",
        "df['Text'] = corrected_texts\n",
        "#print (df['Text'])\n",
        "\n",
        "#print (corrected_texts)\n",
        "# Flatten the lists to a list with sentences\n",
        "corr_strings = [item for sublist in corrected_texts for item in sublist]\n",
        "#print (corr_strings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Creating RoBERTa embeddings for the trainset\n",
        "# Load pre-trained RoBERTa model and tokenizer \n",
        "roberta_model = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Use GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "roberta_model.to(device)\n",
        "\n",
        "# Function to get sentence embeddings for a list of text strings\n",
        "def get_sentence_embeddings(text_list, batch_size=32):\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(text_list), batch_size): # Process the text list in batches\n",
        "        batch = text_list[i:i + batch_size]\n",
        "\n",
        "        # Tokenize the batch of text strings and convert to tensors\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "        # No gradient calculation in forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = roberta_model(**inputs)\n",
        "\n",
        "        # Extract the hidden state of the CLS token (first token) from the last layer\n",
        "        cls_embeddings = outputs.hidden_states[-1][:, 0, :] \n",
        "        all_embeddings.append(cls_embeddings)   # Append the embeddings to the embedding list\n",
        " \n",
        "    # Concatenate all batch embeddings\n",
        "    sentence_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    \n",
        "    return sentence_embeddings\n",
        "\n",
        "# Example usage with strings_dev\n",
        "# sentence_embeddings_dev = get_sentence_embeddings(strings_dev)\n",
        "# print(\"Sentence embeddings shape:\", sentence_embeddings_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Initialize embeddings\n",
        "sentence_embeddings = get_sentence_embeddings(corr_strings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the devset\n",
        "file_path_dev = '/Users/diana/Desktop/isear-validation.xlsx'\n",
        "custom_headers_dev = ['Emotions_dev', 'Text_dev']\n",
        "df_dev = pd.read_excel(file_path_dev, skiprows=1, header=None, names=custom_headers_dev)\n",
        "#print (type(df_dev['Text_dev']))\n",
        "\n",
        "# Data and label processing for devset\n",
        "strings_dev = []\n",
        "for index, sentence in enumerate(df_dev['Text_dev']):\n",
        "    emotion_label_dev = df_dev['Emotions_dev'][index]\n",
        "    strings_dev.append (sentence)\n",
        "#print (strings_dev[0:3])\n",
        "\n",
        "# Label encoding\n",
        "y_dev = df_dev['Emotions_dev'].values\n",
        "y_dev_encoded = np.array([label_encoding[label] for label in y_dev])\n",
        "y_dev_tensor = torch.tensor (y_dev_encoded)\n",
        "#print (len (strings_dev))\n",
        "#print (y_dev_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1150\n"
          ]
        }
      ],
      "source": [
        "# Load the test set\n",
        "file_path_test = '/Users/diana/Desktop/isear-test.xlsx'\n",
        "custom_headers_test = ['Emotions_test', 'Text_test']\n",
        "df_test = pd.read_excel(file_path_test, skiprows=1, header=None, names=custom_headers_test)\n",
        "\n",
        "# Data and label processing for testset\n",
        "strings_test = []\n",
        "for index, sentence in enumerate(df_test['Text_test']):\n",
        "    emotion_label_test = df_test['Emotions_test'][index]\n",
        "    strings_test.append (sentence)\n",
        "\n",
        "strings_test = df_test['Text_test'].tolist()\n",
        "#print (strings_test[0:3])\n",
        "\n",
        "# Label encoding\n",
        "y_test = df_test['Emotions_test'].values\n",
        "y_test_encoded = np.array([label_encoding[label] for label in y_test])\n",
        "y_test_tensor = torch.tensor (y_test_encoded)\n",
        "print (len (df_test['Text_test']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking misspelled words in the testset\n",
        "# Convert the sentences to a single string\n",
        "text_test = ' '.join(df_test['Text_test'].astype(str))\n",
        "\n",
        "# Tokenize the testset \n",
        "tokenized_text_test = []\n",
        "for index, sentence in enumerate(df_test['Text_test']):\n",
        "    emotion_label = df_test['Emotions_test'][index]\n",
        "    tokenized_sentence = tokenize(sentence)\n",
        "    tokenized_text_test.append(tokenized_sentence)\n",
        "\n",
        "filtered_tokens_test = []\n",
        "\n",
        "# Filter out punctuation and digit tokens\n",
        "for tokens in tokenized_text_test:\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        if all(char in string.punctuation for char in token):\n",
        "            continue  # Skip punctuation tokens\n",
        "        if token.isdigit():\n",
        "            continue  # Skip digit tokens\n",
        "        else:\n",
        "            clean_tokens.append(token)\n",
        "    filtered_tokens_test.append(clean_tokens)\n",
        "\n",
        "#print (filtered_tokens_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize spellchecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Flatten the list of lists into a single list\n",
        "just_tokens_test = [token for sentence in filtered_tokens_test for token in sentence]\n",
        "#Count all tokens\n",
        "token_counts_test = Counter(just_tokens_test)\n",
        "\n",
        "# Find the tokens that appear only once\n",
        "uncommon_words_test = [token for token in token_counts_test if token_counts_test[token] == 1]\n",
        "#Check for misspellings\n",
        "misspelled_words_test = spell.unknown(uncommon_words_test)\n",
        "\n",
        "#print(f\"Misspelled words: {misspelled_words_test}\")\n",
        "corrections_test = {word: spell.correction(word) for word in misspelled_words_test}\n",
        "#print(f\"Misspelled words and corrections: {corrections_test}\")\n",
        "#print (len(misspelled_words_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply spelling corrections to each sentence in the text\n",
        "corrected_texts_test = [correct_spelling([sentence], corrections_test) for sentence in filtered_tokens_test]\n",
        "#print (len (corrected_texts_test))\n",
        "\n",
        "corr_strings_test = df_test['Text_test']\n",
        "corr_strings_test = [item for sublist in corrected_texts_test for item in sublist]\n",
        "#print (corr_strings_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence embeddings shape: torch.Size([1150, 768])\n"
          ]
        }
      ],
      "source": [
        "# Creating devset embeddings\n",
        "sentence_embeddings_dev = get_sentence_embeddings(strings_dev)\n",
        "# Print the shape of the embeddings\n",
        "print(\"Sentence embeddings shape:\", sentence_embeddings_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creating testset embeddings\n",
        "sentence_embeddings_test = get_sentence_embeddings(corr_strings_test)\n",
        "#print(\"Sentence embeddings shape:\", sentence_embeddings_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1150]) torch.Size([1150]) torch.Size([5366])\n"
          ]
        }
      ],
      "source": [
        "# Checking dimensions\n",
        "#print (type (sentence_embeddings))\n",
        "y_train_tensor = torch.tensor(y_train_encoded)\n",
        "y_test_tensor = torch.tensor(y_test_encoded)\n",
        "print (y_dev_tensor.shape, y_test_tensor.shape,y_train_tensor.shape) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_11831/574596504.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_train = torch.tensor(sentence_embeddings, dtype=torch.float32).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_11831/574596504.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_test = torch.tensor(sentence_embeddings_test, dtype=torch.float32).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_11831/574596504.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_dev = torch.tensor(sentence_embeddings_dev, dtype=torch.float32).to(device)\n"
          ]
        }
      ],
      "source": [
        "# Adding a disgust embedding feature\n",
        "\n",
        "corr_strings = pd.Series(corr_strings)\n",
        "corr_strings_test = pd.Series(corr_strings_test)\n",
        "\n",
        "# List of unique disgust words identified earlier\n",
        "unique_disgust_words = ['prostitute', 'detested', 'disgusting', 'disgusted', 'disgust', 'vomit', 'rotten', 'stink', 'spider', 'contempt', 'drunk', 'revolt']\n",
        "\n",
        "# Function to check if any of the disgust words are in the text\n",
        "\n",
        "def disgust_words(corrtext, disgust_words):\n",
        "    tokens = word_tokenize(corrtext.lower())\n",
        "    \"\"\"if any(word in tokens for word in disgust_words):\n",
        "        #print(f\"yes: {tokens}\")\n",
        "        matches = sum(1 for word in tokens if word in disgust_words)\n",
        "        #print (matches)\"\"\"\n",
        "    return int(any(word in tokens for word in disgust_words))\n",
        "\n",
        "\n",
        "# Apply the function to create the disgust feature\n",
        "df['disgust_feature_train'] = (corr_strings).apply(lambda x: disgust_words(x, unique_disgust_words))\n",
        "df_test['disgust_feature_test'] = (corr_strings_test).apply(lambda x: disgust_words(x, unique_disgust_words))\n",
        "df_dev['disgust_feature_dev'] = (df_dev['Text_dev']).apply(lambda x: disgust_words(x, unique_disgust_words))\n",
        "\n",
        "\n",
        "# Extract the binary disgust feature as a numpy array and convert to tensor\n",
        "disgust_features_train = torch.tensor(df['disgust_feature_train'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "disgust_features_test = torch.tensor(df_test['disgust_feature_test'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "disgust_features_dev = torch.tensor(df_dev['disgust_feature_dev'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Assuming sentence_embeddings, sentence_embeddings_test, and sentence_embeddings_dev are already defined\n",
        "embeddings_train = torch.tensor(sentence_embeddings, dtype=torch.float32).to(device)\n",
        "embeddings_test = torch.tensor(sentence_embeddings_test, dtype=torch.float32).to(device)\n",
        "embeddings_dev = torch.tensor(sentence_embeddings_dev, dtype=torch.float32).to(device)\n",
        "\n",
        "# Ensure the lengths match before concatenation\n",
        "assert embeddings_train.shape[0] == disgust_features_train.shape[0], \"Train set sizes do not match\"\n",
        "assert embeddings_test.shape[0] == disgust_features_test.shape[0], \"Test set sizes do not match\"\n",
        "assert embeddings_dev.shape[0] == disgust_features_dev.shape[0], \"Dev set sizes do not match\"\n",
        "\n",
        "# Concatenate the RoBERTa embeddings with the disgust feature\n",
        "augmented_embeddings_train = torch.cat((embeddings_train, disgust_features_train), dim=1)\n",
        "augmented_embeddings_test = torch.cat((embeddings_test, disgust_features_test), dim=1)\n",
        "augmented_embeddings_dev = torch.cat((embeddings_dev, disgust_features_dev), dim=1)\n",
        "\n",
        "#print(augmented_embeddings_train.shape[1])\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "770\n"
          ]
        }
      ],
      "source": [
        "# Adding an anger embedding feature\n",
        "unique_anger_words = ['anger', 'angry', 'rage', 'wrath', 'furious', 'enraged', 'irritated', 'kick', 'abuse']\n",
        "\n",
        "# Function to check if any of the anger words are in the text\n",
        "def anger_words(corrtext, anger_words):\n",
        "    tokens = word_tokenize(corrtext.lower())\n",
        "    \"\"\"if any(word in tokens for word in anger_words):\n",
        "        #print(f\"yes: {tokens}\")\"\"\"\n",
        "    return int(any(word in tokens for word in anger_words))\n",
        "\n",
        "# Apply the function to create the anger feature\n",
        "df['anger_feature_train'] = (corr_strings).apply(lambda x: anger_words(x, unique_anger_words))\n",
        "df_test['anger_feature_test'] = (corr_strings_test).apply(lambda x: anger_words(x, unique_anger_words))\n",
        "df_dev['anger_feature_dev'] = df_dev['Text_dev'].apply(lambda x: anger_words(x, unique_anger_words))\n",
        "\n",
        "# Extract the binary anger feature as a numpy array and convert to tensor\n",
        "anger_features_train = torch.tensor(df['anger_feature_train'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "anger_features_test = torch.tensor(df_test['anger_feature_test'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "anger_features_dev = torch.tensor(df_dev['anger_feature_dev'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Ensure the lengths match before concatenation\n",
        "assert embeddings_train.shape[0] == anger_features_train.shape[0], \"Train set sizes do not match\"\n",
        "assert embeddings_test.shape[0] == anger_features_test.shape[0], \"Test set sizes do not match\"\n",
        "assert embeddings_dev.shape[0] == anger_features_dev.shape[0], \"Dev set sizes do not match\"\n",
        "\n",
        "# Concatenate the RoBERTa embeddings with the anger feature\n",
        "augmented_embeddings_train_a = torch.cat((augmented_embeddings_train, anger_features_train), dim=1)\n",
        "augmented_embeddings_test_a = torch.cat((augmented_embeddings_test, anger_features_test), dim=1)\n",
        "augmented_embeddings_dev_a = torch.cat((augmented_embeddings_dev, anger_features_dev), dim=1)\n",
        "\n",
        "print(augmented_embeddings_train_a.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "771\n"
          ]
        }
      ],
      "source": [
        "# Adding a shame embedding feature\n",
        "unique_shame_words = ['ashamed', 'embarrassed', 'humiliated', 'shame', 'regret', 'disgrace', 'dishonor', 'reject']\n",
        "\n",
        "# Function to check if any of the shame words are in the text\n",
        "def shame_words(corrtext, shame_words):\n",
        "    tokens = word_tokenize(corrtext.lower())\n",
        "    \"\"\"if any(word in tokens for word in shame_words):\n",
        "        print(f\"yes\")\"\"\"\n",
        "    return int(any(word in tokens for word in shame_words))\n",
        "\n",
        "# Apply the function to create the shame feature\n",
        "df['shame_feature_train'] = (corr_strings).apply(lambda x: shame_words(x, unique_shame_words))\n",
        "df_test['shame_feature_test'] = (corr_strings_test).apply(lambda x: shame_words(x, unique_shame_words))\n",
        "df_dev['shame_feature_dev'] = df_dev['Text_dev'].apply(lambda x: shame_words(x, unique_shame_words))\n",
        "\n",
        "# Extract the binary shame feature as a numpy array and convert to tensor\n",
        "shame_features_train = torch.tensor(df['shame_feature_train'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "shame_features_test = torch.tensor(df_test['shame_feature_test'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "shame_features_dev = torch.tensor(df_dev['shame_feature_dev'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Ensure the lengths match before concatenation\n",
        "assert embeddings_train.shape[0] == shame_features_train.shape[0], \"Train set sizes do not match\"\n",
        "assert embeddings_test.shape[0] == shame_features_test.shape[0], \"Test set sizes do not match\"\n",
        "assert embeddings_dev.shape[0] == shame_features_dev.shape[0], \"Dev set sizes do not match\"\n",
        "\n",
        "# Concatenate the RoBERTa embeddings with the shame feature\n",
        "augmented_embeddings_train_shame = torch.cat((augmented_embeddings_train_a, shame_features_train), dim=1)\n",
        "augmented_embeddings_test_shame = torch.cat((augmented_embeddings_test_a, shame_features_test), dim=1)\n",
        "augmented_embeddings_dev_shame = torch.cat((augmented_embeddings_dev_a, shame_features_dev), dim=1)\n",
        "\n",
        "print(augmented_embeddings_train_shame.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "772\n"
          ]
        }
      ],
      "source": [
        "# Adding a feature for fear\n",
        "#List of unique fear words identified earlier\n",
        "unique_fear_words = ['afraid', 'fear', 'panic', 'scare', 'worry', 'anxious', 'frighten', 'dark']\n",
        "\n",
        "# Function to check if any of the disgust words are in the text\n",
        "def fear_words(text, fear_words):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    \"\"\"if any(word in tokens for word in fear_words):\n",
        "        #print (\"yes\")\"\"\"\n",
        "    return int(any(word in tokens for word in fear_words))\n",
        "    \n",
        "\n",
        "# Apply the function to create the disgust feature\n",
        "df['fear_feature_train'] = (corr_strings).apply(lambda x: fear_words(x, unique_fear_words))\n",
        "df_test['fear_feature_test'] = (corr_strings_test).apply(lambda x: fear_words(x, unique_fear_words))\n",
        "df_dev['fear_feature_dev'] = df_dev['Text_dev'].apply(lambda x: fear_words(x, unique_fear_words))\n",
        "\n",
        "# Extract the binary disgust feature as a numpy array and convert to tensor\n",
        "fear_features_train = torch.tensor(df['fear_feature_train'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "fear_features_test = torch.tensor(df_test['fear_feature_test'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "fear_features_dev = torch.tensor(df_dev['fear_feature_dev'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Ensure the lengths match before concatenation\n",
        "assert embeddings_train.shape[0] == fear_features_train.shape[0], \"Train set sizes do not match\"\n",
        "assert embeddings_test.shape[0] == fear_features_test.shape[0], \"Test set sizes do not match\"\n",
        "assert embeddings_dev.shape[0] == fear_features_dev.shape[0], \"Dev set sizes do not match\"\n",
        "\n",
        "# Concatenate the RoBERTa embeddings with the anger feature\n",
        "augmented_embeddings_train_fear = torch.cat((augmented_embeddings_train_shame, fear_features_train), dim=1)\n",
        "augmented_embeddings_test_fear= torch.cat((augmented_embeddings_test_shame, fear_features_test), dim=1)\n",
        "augmented_embeddings_dev_fear = torch.cat((augmented_embeddings_dev_shame, fear_features_dev), dim=1)\n",
        "\n",
        "print (augmented_embeddings_train_fear.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_11831/3014603393.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings = torch.tensor(augmented_embeddings_train_fear, dtype=torch.float32).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_11831/3014603393.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_dev = torch.tensor(augmented_embeddings_dev_fear, dtype=torch.float32).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_11831/3014603393.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_test = torch.tensor(augmented_embeddings_test_fear, dtype=torch.float32).to(device)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "embeddings = torch.tensor(augmented_embeddings_train_fear, dtype=torch.float32).to(device)\n",
        "labels = torch.tensor(y_train_encoded, dtype=torch.long).to(device)\n",
        "\n",
        "embeddings_dev = torch.tensor(augmented_embeddings_dev_fear, dtype=torch.float32).to(device)\n",
        "labels_dev = torch.tensor(y_dev_encoded, dtype=torch.long)\n",
        "\n",
        "embeddings_test = torch.tensor(augmented_embeddings_test_fear, dtype=torch.float32).to(device)\n",
        "labels_test = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# Create datasets and dataloaders for training, validation, and testing\n",
        "dataset = TensorDataset(embeddings, labels)\n",
        "dataset_dev = TensorDataset(embeddings_dev,labels_dev)\n",
        "dataset_test = TensorDataset(embeddings_test,labels_test)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(dataset_dev, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim3)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "        self.dropout3 = nn.Dropout(0.4) \n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
        "\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights and biases using Kaiming initialization and constant zero for biases\n",
        "        init.kaiming_uniform_(self.fc1.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc1.bias, 0)\n",
        "        init.kaiming_uniform_(self.fc2.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc2.bias, 0)\n",
        "        init.kaiming_uniform_(self.fc3.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc3.bias, 0)\n",
        "        init.kaiming_uniform_(self.fc4.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc4.bias, 0)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "    \n",
        "# Define dimensions for the neural network\n",
        "input_dim = sentence_embeddings.shape[1] + 4\n",
        "hidden_dim1 = 568  \n",
        "hidden_dim2 = 284   \n",
        "hidden_dim3 = 16\n",
        "output_dim = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instan of the model\n",
        "simp_model = SimpleNN(input_dim, hidden_dim1, output_dim).to(device)\n",
        "# Define Loss and Optimisation function\n",
        "lossf = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW (simp_model.parameters(), lr=0.0001, weight_decay=1e-7) # L2 regularization \n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Train Loss: 2.2262, Val Loss: 2.5837, Test Loss: 1.6403\n",
            "Epoch [2/30], Train Loss: 1.9125, Val Loss: 2.3065, Test Loss: 1.5482\n",
            "Epoch [3/30], Train Loss: 1.7637, Val Loss: 2.2471, Test Loss: 1.4734\n",
            "Epoch [4/30], Train Loss: 1.6756, Val Loss: 2.0338, Test Loss: 1.4286\n",
            "Epoch [5/30], Train Loss: 1.6136, Val Loss: 1.9632, Test Loss: 1.3632\n",
            "Epoch [6/30], Train Loss: 1.5576, Val Loss: 1.8797, Test Loss: 1.3377\n",
            "Epoch [7/30], Train Loss: 1.5099, Val Loss: 1.7896, Test Loss: 1.3076\n",
            "Epoch [8/30], Train Loss: 1.4884, Val Loss: 1.7096, Test Loss: 1.2750\n",
            "Epoch [9/30], Train Loss: 1.4329, Val Loss: 1.6257, Test Loss: 1.2478\n",
            "Epoch [10/30], Train Loss: 1.4157, Val Loss: 1.7119, Test Loss: 1.2067\n",
            "Epoch [11/30], Train Loss: 1.3965, Val Loss: 1.7001, Test Loss: 1.1957\n",
            "Epoch [12/30], Train Loss: 1.3629, Val Loss: 1.6459, Test Loss: 1.1789\n",
            "Epoch [13/30], Train Loss: 1.3434, Val Loss: 1.5714, Test Loss: 1.1759\n",
            "Epoch [14/30], Train Loss: 1.3271, Val Loss: 1.6375, Test Loss: 1.1428\n",
            "Epoch [15/30], Train Loss: 1.2887, Val Loss: 1.5649, Test Loss: 1.1475\n",
            "Epoch [16/30], Train Loss: 1.2951, Val Loss: 1.6256, Test Loss: 1.1274\n",
            "Epoch [17/30], Train Loss: 1.2778, Val Loss: 1.6027, Test Loss: 1.1051\n",
            "Epoch [18/30], Train Loss: 1.2602, Val Loss: 1.5894, Test Loss: 1.0954\n",
            "Epoch [19/30], Train Loss: 1.2357, Val Loss: 1.7206, Test Loss: 1.0771\n",
            "Epoch [20/30], Train Loss: 1.2080, Val Loss: 1.6961, Test Loss: 1.0707\n",
            "Epoch [21/30], Train Loss: 1.2129, Val Loss: 1.6697, Test Loss: 1.0716\n",
            "Epoch [22/30], Train Loss: 1.2011, Val Loss: 1.6918, Test Loss: 1.0557\n",
            "Epoch [23/30], Train Loss: 1.1891, Val Loss: 1.6335, Test Loss: 1.0657\n",
            "Epoch [24/30], Train Loss: 1.1944, Val Loss: 1.5841, Test Loss: 1.0643\n",
            "Epoch [25/30], Train Loss: 1.1810, Val Loss: 1.6444, Test Loss: 1.0606\n",
            "Epoch [26/30], Train Loss: 1.1800, Val Loss: 1.6378, Test Loss: 1.0571\n",
            "Epoch [27/30], Train Loss: 1.1906, Val Loss: 1.6327, Test Loss: 1.0485\n",
            "Epoch [28/30], Train Loss: 1.1727, Val Loss: 1.6943, Test Loss: 1.0525\n",
            "Epoch [29/30], Train Loss: 1.1898, Val Loss: 1.6331, Test Loss: 1.0549\n",
            "Epoch [30/30], Train Loss: 1.1824, Val Loss: 1.7001, Test Loss: 1.0527\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "\n",
        "# Initialize lists to store losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    simp_model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for batch_embeddings, batch_labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        loss = lossf(outputs, batch_labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    # Calculate average training loss\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validation phase\n",
        "    simp_model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_embeddings, batch_labels in val_loader:\n",
        "            outputs = simp_model(batch_embeddings)\n",
        "            loss = lossf(outputs, batch_labels)\n",
        "            val_loss += loss.item()\n",
        "    \n",
        "    # Calculate average validation loss\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Test phase\n",
        "    simp_model.eval()\n",
        "    test_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_embeddings, batch_labels in test_loader:\n",
        "            outputs = simp_model(batch_embeddings)\n",
        "            loss = lossf(outputs, batch_labels)\n",
        "            test_loss += loss.item()\n",
        "    \n",
        "    # Calculate average test loss\n",
        "    test_loss /= len(test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6423343323343323 0.6128966271823415\n",
            "Accuracy of the model on the data: 64.78%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "simp_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_embeddings, batch_labels in test_loader:\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_labels.size(0)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "    f1 = f1_score(predicted , batch_labels, average='weighted')\n",
        "    f1_macro = f1_score(predicted, batch_labels, average='macro')\n",
        "    print (f1, f1_macro)\n",
        "    print(f'Accuracy of the model on the data: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score for class 0: 0.5627\n",
            "F1 score for class 1: 0.6604\n",
            "F1 score for class 2: 0.7781\n",
            "F1 score for class 3: 0.5407\n",
            "F1 score for class 4: 0.8150\n",
            "F1 score for class 5: 0.7101\n",
            "F1 score for class 6: 0.4621\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "simp_model.eval()\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_embeddings, batch_labels in test_loader:\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predicted_labels.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_scores = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Print F1 score for each class\n",
        "for label, f1 in enumerate(f1_scores):\n",
        "    print(f\"F1 score for class {label}: {f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
