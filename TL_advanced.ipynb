{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "SQgYNX8hfFqP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loading the training file\n",
        "file_path = '/Users/diana/Desktop/isear-train.xlsx'\n",
        "custom_headers = ['Emotions', 'Text']\n",
        "df = pd.read_excel(file_path, skiprows=1, header=None, names=custom_headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded labels: ['sadness' 'disgust' 'fear' ... 'sadness' 'disgust' 'fear'] [5 1 2 6 0 6 1 4 6 4 1 0 0 4 2 4 6 2 1 5]\n"
          ]
        }
      ],
      "source": [
        "label_encoding = {'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n",
        "y = df['Emotions'].values\n",
        "y_train_encoded = np.array([label_encoding[label] for label in y])\n",
        "print(\"Encoded labels:\", y, y_train_encoded[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "#conda install torch transformers openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/diana/miniconda3/envs/newenv/lib/python3.10/site-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/diana/miniconda3/envs/newenv/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip  install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "['Losing my girlfriend who made an end to our relationship. By this I lost an important source of support, certainty and joyful moments. I cried very intensly when that happened.', '[ No response.]', 'Staying alone in a dark place.']\n"
          ]
        }
      ],
      "source": [
        "print (type(df['Text']))\n",
        "strings = []\n",
        "for index, sentence in enumerate(df['Text']):\n",
        "    emotion_label = df['Emotions'][index]\n",
        "    strings.append (sentence)\n",
        "print (strings[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence embeddings shape: torch.Size([5366, 768])\n"
          ]
        }
      ],
      "source": [
        "#Training Data BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "def get_sentence_embeddings(text_list, batch_size=32):\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Process each sublist separately\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "        \n",
        "        # Extract the sentence embeddings for the batch\n",
        "        batch_embeddings = outputs.pooler_output\n",
        "        all_embeddings.append(batch_embeddings.cpu())  # Move to CPU to save GPU memory\n",
        "    \n",
        "    # Concatenate all batch embeddings\n",
        "    sentence_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    \n",
        "    return sentence_embeddings\n",
        "\n",
        "# Get embeddings\n",
        "sentence_embeddings = get_sentence_embeddings(strings)\n",
        "\n",
        "# Print the shape of the embeddings\n",
        "print(\"Sentence embeddings shape:\", sentence_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "['[ Laziness makes one sad.]', 'I had to leave my girlfriend for 4 weeks because of my studies. This happens frequently. The last 24 hours before my departure are always very depressing.', 'I promised to join a meeting but did not turn up as I did not want to do the work.']\n"
          ]
        }
      ],
      "source": [
        "#Devset BERT\n",
        "file_path_dev = '/Users/diana/Desktop/isear-validation.xlsx'\n",
        "custom_headers_dev = ['Emotions_dev', 'Text_dev']\n",
        "df = pd.read_excel(file_path_dev, skiprows=1, header=None, names=custom_headers_dev)\n",
        "\n",
        "print (type(df['Text_dev']))\n",
        "strings_dev = []\n",
        "for index, sentence in enumerate(df['Text_dev']):\n",
        "    emotion_label_dev = df['Emotions_dev'][index]\n",
        "    strings_dev.append (sentence)\n",
        "print (strings_dev[0:3])\n",
        "\n",
        "strings_dev = df['Text_dev'].tolist()\n",
        "\n",
        "y_dev = df['Emotions_dev'].values\n",
        "y_dev_encoded = np.array([label_encoding[label] for label in y_dev])\n",
        "y_dev_encoded = torch.tensor (y_dev_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence embeddings shape: torch.Size([1150, 768])\n"
          ]
        }
      ],
      "source": [
        "sentence_embeddings_dev = get_sentence_embeddings(strings_dev)\n",
        "# Print the shape of the embeddings\n",
        "print(\"Sentence embeddings shape:\", sentence_embeddings_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(sentence_embeddings, \"sentence_embeddings.pt\")\n",
        "# To load the embeddings later\n",
        "loaded_embeddings = torch.load(\"sentence_embeddings.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.8667, -0.5060, -0.9691,  ..., -0.8864, -0.6726,  0.8883],\n",
            "        [-0.9396, -0.5441, -0.9673,  ..., -0.8054, -0.6967,  0.9153],\n",
            "        [-0.8893, -0.4814, -0.9798,  ..., -0.9239, -0.7393,  0.8747],\n",
            "        ...,\n",
            "        [-0.8967, -0.5091, -0.9448,  ..., -0.8054, -0.7148,  0.8998],\n",
            "        [-0.8920, -0.4614, -0.8404,  ..., -0.7408, -0.6178,  0.9071],\n",
            "        [-0.8762, -0.5444, -0.9822,  ..., -0.9704, -0.6663,  0.9188]])\n"
          ]
        }
      ],
      "source": [
        "print (sentence_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegressionMulticlass:\n",
        "    def __init__(self, learning_rate=0.3, num_iterations=10000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "    \n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "    \n",
        "    def xavier_init(self, shape):\n",
        "        fan_in = shape[0]\n",
        "        fan_out = shape[1]\n",
        "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "        return np.random.uniform(-limit, limit, size=shape)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        # Convert PyTorch tensors to NumPy arrays\n",
        "        if isinstance(X, torch.Tensor):\n",
        "            X = X.detach().cpu().numpy()\n",
        "        if isinstance(y, torch.Tensor):\n",
        "            y = y.detach().cpu().numpy()\n",
        "\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "        self.weights = self.xavier_init((num_features, num_classes))\n",
        "        self.bias = np.zeros((1, num_classes))\n",
        "        y_one_hot = np.eye(num_classes)[y]\n",
        "        \n",
        "        for _ in range(self.num_iterations):\n",
        "            linear_model = X.dot(self.weights) + self.bias\n",
        "            y_pred = self.softmax(linear_model)\n",
        "            \n",
        "            dw = (1 / num_samples) * X.T.dot(y_pred - y_one_hot)\n",
        "            db = (1 / num_samples) * np.sum(y_pred - y_one_hot, axis=0, keepdims=True)\n",
        "            \n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if isinstance(X, torch.Tensor):\n",
        "            X = X.detach().cpu().numpy()\n",
        "\n",
        "        num_samples, num_features = X.shape\n",
        "        _, num_classes = self.weights.shape\n",
        "    \n",
        "        if num_features != self.weights.shape[0]:\n",
        "            self.weights = self.xavier_init((num_features, num_classes))\n",
        "            \n",
        "        linear_predictions = X.dot(self.weights) + self.bias\n",
        "        y_pred = self.softmax(linear_predictions)\n",
        "        print(self.weights.shape)\n",
        "        class_pred = np.argmax(y_pred, axis=1)  # Choose the class with the highest probability\n",
        "        return class_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "print (type (sentence_embeddings))\n",
        "y_train_tensor = torch.tensor(y_train_encoded)\n",
        "print (type (y_train_tensor)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_emotions = LogisticRegressionMulticlass()\n",
        "model_emotions.fit (sentence_embeddings, y_train_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(768, 7)\n",
            "torch.Size([5366, 768])\n",
            "Micro-average F1-score: 0.3438315318673127\n",
            "F1-score on training data: 0.4214195922893096\n"
          ]
        }
      ],
      "source": [
        "pred = model_emotions.predict (sentence_embeddings)\n",
        "\n",
        "print(sentence_embeddings.shape)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "micro_average_f1 = f1_score(pred, y_train_encoded, average='micro')\n",
        "print(\"Micro-average F1-score:\", micro_average_f1)\n",
        "\n",
        "f1_external = f1_score(pred, y_train_encoded, average='weighted')\n",
        "print(\"F1-score on training data:\", f1_external)\n",
        "\n",
        "#print (pred [0:300],y_train_encoded [0:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_71965/2679204248.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_71965/2679204248.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(y_train_tensor, dtype=torch.long).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_71965/2679204248.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_dev = torch.tensor(sentence_embeddings_dev, dtype=torch.float32)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_71965/2679204248.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_dev = torch.tensor(y_dev_encoded, dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32).to(device)\n",
        "labels = torch.tensor(y_train_tensor, dtype=torch.long).to(device)\n",
        "\n",
        "embeddings_dev = torch.tensor(sentence_embeddings_dev, dtype=torch.float32)\n",
        "labels_dev = torch.tensor(y_dev_encoded, dtype=torch.long)\n",
        "\n",
        "# Create a dataset and dataloader\n",
        "dataset = TensorDataset(embeddings, labels)\n",
        "dataset_dev = TensorDataset(embeddings_dev,labels_dev)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset_dev, batch_size=32, shuffle=False)\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "input_dim = 768  # Dimension of BERT embeddings\n",
        "hidden_dim = 128\n",
        "output_dim = 7  # Number of emotion classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [],
      "source": [
        "simp_model = SimpleNN(input_dim, hidden_dim, output_dim).to(device)\n",
        "lossf = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(simp_model.parameters(), lr=0.0005, weight_decay=1e-6) # L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/300], Loss: 1.9123\n",
            "Epoch [2/300], Loss: 1.8617\n",
            "Epoch [3/300], Loss: 1.8971\n",
            "Epoch [4/300], Loss: 1.7225\n",
            "Epoch [5/300], Loss: 1.5977\n",
            "Epoch [6/300], Loss: 1.6715\n",
            "Epoch [7/300], Loss: 1.8330\n",
            "Epoch [8/300], Loss: 1.7793\n",
            "Epoch [9/300], Loss: 1.5355\n",
            "Epoch [10/300], Loss: 1.5455\n",
            "Epoch [11/300], Loss: 1.4638\n",
            "Epoch [12/300], Loss: 1.1661\n",
            "Epoch [13/300], Loss: 1.3389\n",
            "Epoch [14/300], Loss: 1.5576\n",
            "Epoch [15/300], Loss: 1.5666\n",
            "Epoch [16/300], Loss: 1.2362\n",
            "Epoch [17/300], Loss: 1.2819\n",
            "Epoch [18/300], Loss: 1.4841\n",
            "Epoch [19/300], Loss: 1.5547\n",
            "Epoch [20/300], Loss: 1.0336\n",
            "Epoch [21/300], Loss: 1.2330\n",
            "Epoch [22/300], Loss: 1.4050\n",
            "Epoch [23/300], Loss: 1.2227\n",
            "Epoch [24/300], Loss: 1.3338\n",
            "Epoch [25/300], Loss: 1.4951\n",
            "Epoch [26/300], Loss: 1.2680\n",
            "Epoch [27/300], Loss: 1.4546\n",
            "Epoch [28/300], Loss: 1.3512\n",
            "Epoch [29/300], Loss: 1.0410\n",
            "Epoch [30/300], Loss: 1.4225\n",
            "Epoch [31/300], Loss: 1.1768\n",
            "Epoch [32/300], Loss: 1.3895\n",
            "Epoch [33/300], Loss: 1.0576\n",
            "Epoch [34/300], Loss: 1.3563\n",
            "Epoch [35/300], Loss: 1.2212\n",
            "Epoch [36/300], Loss: 1.4542\n",
            "Epoch [37/300], Loss: 1.2559\n",
            "Epoch [38/300], Loss: 1.4797\n",
            "Epoch [39/300], Loss: 1.4192\n",
            "Epoch [40/300], Loss: 1.0630\n",
            "Epoch [41/300], Loss: 1.0648\n",
            "Epoch [42/300], Loss: 1.2630\n",
            "Epoch [43/300], Loss: 1.1520\n",
            "Epoch [44/300], Loss: 1.5448\n",
            "Epoch [45/300], Loss: 1.0325\n",
            "Epoch [46/300], Loss: 0.9567\n",
            "Epoch [47/300], Loss: 1.5291\n",
            "Epoch [48/300], Loss: 0.9228\n",
            "Epoch [49/300], Loss: 1.1371\n",
            "Epoch [50/300], Loss: 1.6474\n",
            "Epoch [51/300], Loss: 1.4251\n",
            "Epoch [52/300], Loss: 1.5805\n",
            "Epoch [53/300], Loss: 1.1595\n",
            "Epoch [54/300], Loss: 1.0459\n",
            "Epoch [55/300], Loss: 1.0565\n",
            "Epoch [56/300], Loss: 1.2595\n",
            "Epoch [57/300], Loss: 0.8969\n",
            "Epoch [58/300], Loss: 1.3865\n",
            "Epoch [59/300], Loss: 1.1626\n",
            "Epoch [60/300], Loss: 1.5527\n",
            "Epoch [61/300], Loss: 0.9249\n",
            "Epoch [62/300], Loss: 1.3415\n",
            "Epoch [63/300], Loss: 1.1619\n",
            "Epoch [64/300], Loss: 1.0136\n",
            "Epoch [65/300], Loss: 1.2560\n",
            "Epoch [66/300], Loss: 1.2191\n",
            "Epoch [67/300], Loss: 1.0105\n",
            "Epoch [68/300], Loss: 0.8907\n",
            "Epoch [69/300], Loss: 1.0448\n",
            "Epoch [70/300], Loss: 1.0920\n",
            "Epoch [71/300], Loss: 1.1707\n",
            "Epoch [72/300], Loss: 1.0302\n",
            "Epoch [73/300], Loss: 1.0225\n",
            "Epoch [74/300], Loss: 1.2116\n",
            "Epoch [75/300], Loss: 1.1854\n",
            "Epoch [76/300], Loss: 1.2130\n",
            "Epoch [77/300], Loss: 1.3268\n",
            "Epoch [78/300], Loss: 1.5093\n",
            "Epoch [79/300], Loss: 1.0401\n",
            "Epoch [80/300], Loss: 1.1750\n",
            "Epoch [81/300], Loss: 1.5687\n",
            "Epoch [82/300], Loss: 1.0027\n",
            "Epoch [83/300], Loss: 0.8813\n",
            "Epoch [84/300], Loss: 1.1966\n",
            "Epoch [85/300], Loss: 1.1032\n",
            "Epoch [86/300], Loss: 1.3055\n",
            "Epoch [87/300], Loss: 0.9129\n",
            "Epoch [88/300], Loss: 1.5065\n",
            "Epoch [89/300], Loss: 1.4979\n",
            "Epoch [90/300], Loss: 1.1102\n",
            "Epoch [91/300], Loss: 1.3158\n",
            "Epoch [92/300], Loss: 1.1421\n",
            "Epoch [93/300], Loss: 0.9800\n",
            "Epoch [94/300], Loss: 1.4924\n",
            "Epoch [95/300], Loss: 1.4021\n",
            "Epoch [96/300], Loss: 1.1781\n",
            "Epoch [97/300], Loss: 0.8826\n",
            "Epoch [98/300], Loss: 0.8867\n",
            "Epoch [99/300], Loss: 1.1314\n",
            "Epoch [100/300], Loss: 0.9507\n",
            "Epoch [101/300], Loss: 0.9122\n",
            "Epoch [102/300], Loss: 1.1264\n",
            "Epoch [103/300], Loss: 1.1546\n",
            "Epoch [104/300], Loss: 1.4099\n",
            "Epoch [105/300], Loss: 1.1441\n",
            "Epoch [106/300], Loss: 1.2559\n",
            "Epoch [107/300], Loss: 1.0780\n",
            "Epoch [108/300], Loss: 1.2204\n",
            "Epoch [109/300], Loss: 1.3261\n",
            "Epoch [110/300], Loss: 1.4256\n",
            "Epoch [111/300], Loss: 1.2228\n",
            "Epoch [112/300], Loss: 1.0158\n",
            "Epoch [113/300], Loss: 1.0954\n",
            "Epoch [114/300], Loss: 1.3170\n",
            "Epoch [115/300], Loss: 1.2018\n",
            "Epoch [116/300], Loss: 1.2261\n",
            "Epoch [117/300], Loss: 1.1619\n",
            "Epoch [118/300], Loss: 0.9146\n",
            "Epoch [119/300], Loss: 1.0483\n",
            "Epoch [120/300], Loss: 1.4591\n",
            "Epoch [121/300], Loss: 0.8536\n",
            "Epoch [122/300], Loss: 1.2183\n",
            "Epoch [123/300], Loss: 1.0563\n",
            "Epoch [124/300], Loss: 1.0095\n",
            "Epoch [125/300], Loss: 0.8696\n",
            "Epoch [126/300], Loss: 0.9974\n",
            "Epoch [127/300], Loss: 0.8618\n",
            "Epoch [128/300], Loss: 0.9106\n",
            "Epoch [129/300], Loss: 1.1165\n",
            "Epoch [130/300], Loss: 1.1429\n",
            "Epoch [131/300], Loss: 1.1966\n",
            "Epoch [132/300], Loss: 0.9379\n",
            "Epoch [133/300], Loss: 1.5006\n",
            "Epoch [134/300], Loss: 0.6121\n",
            "Epoch [135/300], Loss: 0.9521\n",
            "Epoch [136/300], Loss: 1.0211\n",
            "Epoch [137/300], Loss: 1.1339\n",
            "Epoch [138/300], Loss: 1.4171\n",
            "Epoch [139/300], Loss: 0.9765\n",
            "Epoch [140/300], Loss: 1.2189\n",
            "Epoch [141/300], Loss: 0.9058\n",
            "Epoch [142/300], Loss: 0.8912\n",
            "Epoch [143/300], Loss: 0.8694\n",
            "Epoch [144/300], Loss: 1.0105\n",
            "Epoch [145/300], Loss: 1.4260\n",
            "Epoch [146/300], Loss: 1.5052\n",
            "Epoch [147/300], Loss: 1.4762\n",
            "Epoch [148/300], Loss: 1.2097\n",
            "Epoch [149/300], Loss: 1.1679\n",
            "Epoch [150/300], Loss: 0.9271\n",
            "Epoch [151/300], Loss: 0.8963\n",
            "Epoch [152/300], Loss: 1.1371\n",
            "Epoch [153/300], Loss: 0.9205\n",
            "Epoch [154/300], Loss: 0.9198\n",
            "Epoch [155/300], Loss: 0.8599\n",
            "Epoch [156/300], Loss: 1.1680\n",
            "Epoch [157/300], Loss: 0.9631\n",
            "Epoch [158/300], Loss: 1.5038\n",
            "Epoch [159/300], Loss: 1.1406\n",
            "Epoch [160/300], Loss: 0.6375\n",
            "Epoch [161/300], Loss: 0.9397\n",
            "Epoch [162/300], Loss: 1.0087\n",
            "Epoch [163/300], Loss: 1.0715\n",
            "Epoch [164/300], Loss: 1.0158\n",
            "Epoch [165/300], Loss: 1.0502\n",
            "Epoch [166/300], Loss: 0.8681\n",
            "Epoch [167/300], Loss: 1.0019\n",
            "Epoch [168/300], Loss: 1.7004\n",
            "Epoch [169/300], Loss: 1.3027\n",
            "Epoch [170/300], Loss: 0.7569\n",
            "Epoch [171/300], Loss: 0.5580\n",
            "Epoch [172/300], Loss: 1.4738\n",
            "Epoch [173/300], Loss: 1.2852\n",
            "Epoch [174/300], Loss: 0.6955\n",
            "Epoch [175/300], Loss: 0.9217\n",
            "Epoch [176/300], Loss: 0.7549\n",
            "Epoch [177/300], Loss: 1.7715\n",
            "Epoch [178/300], Loss: 0.7411\n",
            "Epoch [179/300], Loss: 1.0960\n",
            "Epoch [180/300], Loss: 0.7785\n",
            "Epoch [181/300], Loss: 0.7031\n",
            "Epoch [182/300], Loss: 1.1468\n",
            "Epoch [183/300], Loss: 1.2780\n",
            "Epoch [184/300], Loss: 1.2015\n",
            "Epoch [185/300], Loss: 0.6636\n",
            "Epoch [186/300], Loss: 0.6275\n",
            "Epoch [187/300], Loss: 0.8346\n",
            "Epoch [188/300], Loss: 1.3173\n",
            "Epoch [189/300], Loss: 1.0855\n",
            "Epoch [190/300], Loss: 0.8433\n",
            "Epoch [191/300], Loss: 1.0078\n",
            "Epoch [192/300], Loss: 1.0461\n",
            "Epoch [193/300], Loss: 0.9010\n",
            "Epoch [194/300], Loss: 1.2564\n",
            "Epoch [195/300], Loss: 1.0711\n",
            "Epoch [196/300], Loss: 1.2838\n",
            "Epoch [197/300], Loss: 1.2085\n",
            "Epoch [198/300], Loss: 1.0881\n",
            "Epoch [199/300], Loss: 1.0891\n",
            "Epoch [200/300], Loss: 1.2366\n",
            "Epoch [201/300], Loss: 0.8782\n",
            "Epoch [202/300], Loss: 0.7040\n",
            "Epoch [203/300], Loss: 0.9403\n",
            "Epoch [204/300], Loss: 1.0132\n",
            "Epoch [205/300], Loss: 0.8931\n",
            "Epoch [206/300], Loss: 1.2778\n",
            "Epoch [207/300], Loss: 1.0793\n",
            "Epoch [208/300], Loss: 0.9558\n",
            "Epoch [209/300], Loss: 1.0508\n",
            "Epoch [210/300], Loss: 1.1390\n",
            "Epoch [211/300], Loss: 1.0329\n",
            "Epoch [212/300], Loss: 1.0534\n",
            "Epoch [213/300], Loss: 1.2834\n",
            "Epoch [214/300], Loss: 1.0912\n",
            "Epoch [215/300], Loss: 1.4488\n",
            "Epoch [216/300], Loss: 1.0856\n",
            "Epoch [217/300], Loss: 0.8762\n",
            "Epoch [218/300], Loss: 1.3280\n",
            "Epoch [219/300], Loss: 1.0557\n",
            "Epoch [220/300], Loss: 0.8480\n",
            "Epoch [221/300], Loss: 1.0450\n",
            "Epoch [222/300], Loss: 0.7597\n",
            "Epoch [223/300], Loss: 1.0267\n",
            "Epoch [224/300], Loss: 1.5010\n",
            "Epoch [225/300], Loss: 1.0101\n",
            "Epoch [226/300], Loss: 0.6111\n",
            "Epoch [227/300], Loss: 0.8936\n",
            "Epoch [228/300], Loss: 0.9066\n",
            "Epoch [229/300], Loss: 1.1915\n",
            "Epoch [230/300], Loss: 1.0614\n",
            "Epoch [231/300], Loss: 0.7089\n",
            "Epoch [232/300], Loss: 1.3657\n",
            "Epoch [233/300], Loss: 1.0024\n",
            "Epoch [234/300], Loss: 1.1059\n",
            "Epoch [235/300], Loss: 0.5999\n",
            "Epoch [236/300], Loss: 0.7353\n",
            "Epoch [237/300], Loss: 1.0761\n",
            "Epoch [238/300], Loss: 1.0900\n",
            "Epoch [239/300], Loss: 1.1006\n",
            "Epoch [240/300], Loss: 1.1123\n",
            "Epoch [241/300], Loss: 0.9499\n",
            "Epoch [242/300], Loss: 0.8760\n",
            "Epoch [243/300], Loss: 0.8149\n",
            "Epoch [244/300], Loss: 0.9002\n",
            "Epoch [245/300], Loss: 0.5783\n",
            "Epoch [246/300], Loss: 1.1287\n",
            "Epoch [247/300], Loss: 1.4490\n",
            "Epoch [248/300], Loss: 0.6534\n",
            "Epoch [249/300], Loss: 0.8816\n",
            "Epoch [250/300], Loss: 0.7148\n",
            "Epoch [251/300], Loss: 0.8613\n",
            "Epoch [252/300], Loss: 0.8865\n",
            "Epoch [253/300], Loss: 1.2635\n",
            "Epoch [254/300], Loss: 1.4526\n",
            "Epoch [255/300], Loss: 0.9167\n",
            "Epoch [256/300], Loss: 0.7161\n",
            "Epoch [257/300], Loss: 0.9938\n",
            "Epoch [258/300], Loss: 0.8308\n",
            "Epoch [259/300], Loss: 1.2120\n",
            "Epoch [260/300], Loss: 0.6153\n",
            "Epoch [261/300], Loss: 0.8359\n",
            "Epoch [262/300], Loss: 1.0304\n",
            "Epoch [263/300], Loss: 0.8917\n",
            "Epoch [264/300], Loss: 1.0360\n",
            "Epoch [265/300], Loss: 1.1699\n",
            "Epoch [266/300], Loss: 0.8807\n",
            "Epoch [267/300], Loss: 0.7173\n",
            "Epoch [268/300], Loss: 0.8696\n",
            "Epoch [269/300], Loss: 0.7048\n",
            "Epoch [270/300], Loss: 0.7134\n",
            "Epoch [271/300], Loss: 1.1432\n",
            "Epoch [272/300], Loss: 0.5817\n",
            "Epoch [273/300], Loss: 1.1782\n",
            "Epoch [274/300], Loss: 1.2155\n",
            "Epoch [275/300], Loss: 1.1704\n",
            "Epoch [276/300], Loss: 0.7924\n",
            "Epoch [277/300], Loss: 0.8481\n",
            "Epoch [278/300], Loss: 0.8204\n",
            "Epoch [279/300], Loss: 1.0014\n",
            "Epoch [280/300], Loss: 0.6181\n",
            "Epoch [281/300], Loss: 0.8587\n",
            "Epoch [282/300], Loss: 0.9605\n",
            "Epoch [283/300], Loss: 1.0676\n",
            "Epoch [284/300], Loss: 0.5528\n",
            "Epoch [285/300], Loss: 0.6168\n",
            "Epoch [286/300], Loss: 0.9261\n",
            "Epoch [287/300], Loss: 0.9059\n",
            "Epoch [288/300], Loss: 0.4973\n",
            "Epoch [289/300], Loss: 0.7665\n",
            "Epoch [290/300], Loss: 0.7202\n",
            "Epoch [291/300], Loss: 0.8310\n",
            "Epoch [292/300], Loss: 1.1535\n",
            "Epoch [293/300], Loss: 0.7043\n",
            "Epoch [294/300], Loss: 1.0296\n",
            "Epoch [295/300], Loss: 0.6906\n",
            "Epoch [296/300], Loss: 0.9162\n",
            "Epoch [297/300], Loss: 0.8273\n",
            "Epoch [298/300], Loss: 0.5954\n",
            "Epoch [299/300], Loss: 1.0554\n",
            "Epoch [300/300], Loss: 1.1748\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 300\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    simp_model.train()\n",
        "    for batch_embeddings, batch_labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        loss = lossf(outputs, batch_labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6320965309200605 0.6333333333333333\n",
            "Accuracy of the model on the data: 56.17%\n"
          ]
        }
      ],
      "source": [
        "simp_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_embeddings, batch_labels in test_loader:\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_labels.size(0)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "    f1 = f1_score(predicted , batch_labels, average='weighted')\n",
        "    f1_micro = f1_score(predicted , batch_labels, average='micro')\n",
        "    print (f1, f1_micro)\n",
        "    print(f'Accuracy of the model on the data: {100 * correct / total:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
