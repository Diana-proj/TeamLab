{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SQgYNX8hfFqP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5366\n"
          ]
        }
      ],
      "source": [
        "# Loading the training file\n",
        "file_path = '/Users/diana/Desktop/isear-train.xlsx'\n",
        "\n",
        "# Define custom headers\n",
        "custom_headers = ['Emotions', 'Text']\n",
        "df = pd.read_excel(file_path, skiprows=1, header=None, names=custom_headers)\n",
        "\n",
        "# Display the first few rows\n",
        "print(len(df['Text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label preprocessing\n",
        "\n",
        "label_encoding = {'anger': 0, 'disgust': 1, 'fear': 2, 'guilt': 3, 'joy': 4, 'sadness': 5, 'shame': 6}\n",
        "y = df['Emotions'].values\n",
        "y_train_encoded = np.array([label_encoding[label] for label in y])\n",
        "y_train_tensor = torch.tensor (y_train_encoded)\n",
        "\n",
        "#print(\"Encoded labels:\", y, y_train_encoded[:20])\n",
        "#print(len(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Creating RoBERTa embeddings for the trainset\n",
        "# Load pre-trained RoBERTa model and tokenizer \n",
        "roberta_model = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Use GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "roberta_model.to(device)\n",
        "\n",
        "# Function to get sentence embeddings for a list of text strings\n",
        "def get_sentence_embeddings(text_list, batch_size=32):\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(text_list), batch_size): # Process the text list in batches\n",
        "        batch = text_list[i:i + batch_size]\n",
        "\n",
        "        # Tokenize the batch of text strings and convert to tensors\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "        # No gradient calculation in forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = roberta_model(**inputs)\n",
        "\n",
        "        # Extract the hidden state of the CLS token (first token) from the last layer\n",
        "        cls_embeddings = outputs.hidden_states[-1][:, 0, :] \n",
        "        all_embeddings.append(cls_embeddings)   # Append the embeddings to the embedding list\n",
        " \n",
        "    # Concatenate all batch embeddings\n",
        "    sentence_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    \n",
        "    return sentence_embeddings\n",
        "\n",
        "# Example usage with strings_dev\n",
        "# sentence_embeddings_dev = get_sentence_embeddings(strings_dev)\n",
        "# print(\"Sentence embeddings shape:\", sentence_embeddings_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize embeddings for trainset\n",
        "sentence_embeddings = get_sentence_embeddings(list(df['Text']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the devset\n",
        "file_path_dev = '/Users/diana/Desktop/isear-validation.xlsx'\n",
        "custom_headers_dev = ['Emotions_dev', 'Text_dev']\n",
        "df_dev = pd.read_excel(file_path_dev, skiprows=1, header=None, names=custom_headers_dev)\n",
        "#print (type(df_dev['Text_dev']))\n",
        "\n",
        "# Data and label processing for devset\n",
        "strings_dev = []\n",
        "for index, sentence in enumerate(df_dev['Text_dev']):\n",
        "    emotion_label_dev = df_dev['Emotions_dev'][index]\n",
        "    strings_dev.append (sentence)\n",
        "#print (strings_dev[0:3])\n",
        "\n",
        "# Label encoding\n",
        "y_dev = df_dev['Emotions_dev'].values\n",
        "y_dev_encoded = np.array([label_encoding[label] for label in y_dev])\n",
        "y_dev_tensor = torch.tensor (y_dev_encoded)\n",
        "#print (len (strings_dev))\n",
        "#print (y_dev_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1150\n"
          ]
        }
      ],
      "source": [
        "# Load the test set\n",
        "file_path_test = '/Users/diana/Desktop/isear-test.xlsx'\n",
        "custom_headers_test = ['Emotions_test', 'Text_test']\n",
        "df_test = pd.read_excel(file_path_test, skiprows=1, header=None, names=custom_headers_test)\n",
        "\n",
        "# Data and label processing for testset\n",
        "strings_test = []\n",
        "for index, sentence in enumerate(df_test['Text_test']):\n",
        "    emotion_label_test = df_test['Emotions_test'][index]\n",
        "    strings_test.append (sentence)\n",
        "\n",
        "strings_test = df_test['Text_test'].tolist()\n",
        "#print (strings_test[0:3])\n",
        "\n",
        "# Label encoding\n",
        "y_test = df_test['Emotions_test'].values\n",
        "y_test_encoded = np.array([label_encoding[label] for label in y_test])\n",
        "y_test_tensor = torch.tensor (y_test_encoded)\n",
        "print (len (df_test['Text_test']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence embeddings shape: torch.Size([1150, 768])\n"
          ]
        }
      ],
      "source": [
        "# Creating devset embeddings\n",
        "sentence_embeddings_dev = get_sentence_embeddings(list(df_dev['Text_dev']))\n",
        "# Print the shape of the embeddings\n",
        "print(\"Sentence embeddings shape:\", sentence_embeddings_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating testset embeddings\n",
        "sentence_embeddings_test = get_sentence_embeddings(list(df_test['Text_test']))\n",
        "#print(\"Sentence embeddings shape:\", sentence_embeddings_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1150]) torch.Size([1150]) torch.Size([5366])\n"
          ]
        }
      ],
      "source": [
        "# Checking dimensions\n",
        "#print (type (sentence_embeddings))\n",
        "y_train_tensor = torch.tensor(y_train_encoded)\n",
        "y_test_tensor = torch.tensor(y_test_encoded)\n",
        "print (y_dev_tensor.shape, y_test_tensor.shape,y_train_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_9267/2591040279.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_9267/2591040279.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_dev = torch.tensor(sentence_embeddings_dev, dtype=torch.float32).to(device)\n",
            "/var/folders/mh/2g8pjxqx2dldlw8dltkfmgvm0000gn/T/ipykernel_9267/2591040279.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embeddings_test = torch.tensor(sentence_embeddings_test, dtype=torch.float32).to(device)\n"
          ]
        }
      ],
      "source": [
        "# Convert to PyTorch tensors\n",
        "embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32).to(device)\n",
        "labels = torch.tensor(y_train_encoded, dtype=torch.long).to(device)\n",
        "\n",
        "embeddings_dev = torch.tensor(sentence_embeddings_dev, dtype=torch.float32).to(device)\n",
        "labels_dev = torch.tensor(y_dev_encoded, dtype=torch.long)\n",
        "\n",
        "embeddings_test = torch.tensor(sentence_embeddings_test, dtype=torch.float32).to(device)\n",
        "labels_test = torch.tensor(y_test_encoded, dtype=torch.long)\n",
        "\n",
        "# Create datasets and dataloaders for training, validation, and testing\n",
        "dataset = TensorDataset(embeddings, labels)\n",
        "dataset_dev = TensorDataset(embeddings_dev,labels_dev)\n",
        "dataset_test = TensorDataset(embeddings_test,labels_test)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(dataset_dev, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
        "\n",
        "class SimpleNN(nn.Module): # Defining the model\n",
        "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim3)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "        self.dropout3 = nn.Dropout(0.4) \n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
        "\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights and biases using Kaiming initialization and constant zero for biases\n",
        "        init.kaiming_uniform_(self.fc1.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc1.bias, 0)\n",
        "        init.kaiming_uniform_(self.fc2.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc2.bias, 0)\n",
        "        init.kaiming_uniform_(self.fc3.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc3.bias, 0)\n",
        "        init.kaiming_uniform_(self.fc4.weight, nonlinearity='leaky_relu')\n",
        "        init.constant_(self.fc4.bias, 0)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "    \n",
        "# Define dimensions for the neural network\n",
        "input_dim = sentence_embeddings.shape[1]\n",
        "hidden_dim1 = 568  \n",
        "hidden_dim2 = 284   \n",
        "hidden_dim3 = 16\n",
        "output_dim = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instant of the model\n",
        "simp_model = SimpleNN(input_dim, hidden_dim1, output_dim).to(device)\n",
        "# Define Loss and Optimisation function\n",
        "lossf = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW (simp_model.parameters(), lr=0.0001, weight_decay=1e-9) # L2 regularization \n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Train Loss: 0.9036, Val Loss: 1.0371, Test Loss: 1.0286\n",
            "Epoch [2/30], Train Loss: 0.8748, Val Loss: 1.0390, Test Loss: 1.0254\n",
            "Epoch [3/30], Train Loss: 0.8862, Val Loss: 1.0413, Test Loss: 1.0257\n",
            "Epoch [4/30], Train Loss: 0.8838, Val Loss: 1.0407, Test Loss: 1.0290\n",
            "Epoch [5/30], Train Loss: 0.9031, Val Loss: 1.0395, Test Loss: 1.0282\n",
            "Epoch [6/30], Train Loss: 0.8863, Val Loss: 1.0407, Test Loss: 1.0270\n",
            "Epoch [7/30], Train Loss: 0.8819, Val Loss: 1.0411, Test Loss: 1.0260\n",
            "Epoch [8/30], Train Loss: 0.8874, Val Loss: 1.0467, Test Loss: 1.0296\n",
            "Epoch [9/30], Train Loss: 0.8900, Val Loss: 1.0371, Test Loss: 1.0311\n",
            "Epoch [10/30], Train Loss: 0.8675, Val Loss: 1.0436, Test Loss: 1.0267\n",
            "Epoch [11/30], Train Loss: 0.8846, Val Loss: 1.0434, Test Loss: 1.0291\n",
            "Epoch [12/30], Train Loss: 0.8959, Val Loss: 1.0444, Test Loss: 1.0273\n",
            "Epoch [13/30], Train Loss: 0.8819, Val Loss: 1.0417, Test Loss: 1.0295\n",
            "Epoch [14/30], Train Loss: 0.8865, Val Loss: 1.0421, Test Loss: 1.0258\n",
            "Epoch [15/30], Train Loss: 0.8735, Val Loss: 1.0377, Test Loss: 1.0256\n",
            "Epoch [16/30], Train Loss: 0.8805, Val Loss: 1.0343, Test Loss: 1.0269\n",
            "Epoch [17/30], Train Loss: 0.8831, Val Loss: 1.0371, Test Loss: 1.0242\n",
            "Epoch [18/30], Train Loss: 0.8871, Val Loss: 1.0376, Test Loss: 1.0261\n",
            "Epoch [19/30], Train Loss: 0.8798, Val Loss: 1.0365, Test Loss: 1.0259\n",
            "Epoch [20/30], Train Loss: 0.8866, Val Loss: 1.0398, Test Loss: 1.0247\n",
            "Epoch [21/30], Train Loss: 0.8837, Val Loss: 1.0396, Test Loss: 1.0267\n",
            "Epoch [22/30], Train Loss: 0.8821, Val Loss: 1.0392, Test Loss: 1.0289\n",
            "Epoch [23/30], Train Loss: 0.8776, Val Loss: 1.0507, Test Loss: 1.0306\n",
            "Epoch [24/30], Train Loss: 0.8970, Val Loss: 1.0386, Test Loss: 1.0284\n",
            "Epoch [25/30], Train Loss: 0.8949, Val Loss: 1.0413, Test Loss: 1.0255\n",
            "Epoch [26/30], Train Loss: 0.8961, Val Loss: 1.0432, Test Loss: 1.0328\n",
            "Epoch [27/30], Train Loss: 0.8829, Val Loss: 1.0389, Test Loss: 1.0263\n",
            "Epoch [28/30], Train Loss: 0.8858, Val Loss: 1.0402, Test Loss: 1.0262\n",
            "Epoch [29/30], Train Loss: 0.8938, Val Loss: 1.0394, Test Loss: 1.0273\n",
            "Epoch [30/30], Train Loss: 0.8937, Val Loss: 1.0376, Test Loss: 1.0247\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "\n",
        "# Initialize lists to store losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    simp_model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for batch_embeddings, batch_labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        loss = lossf(outputs, batch_labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    # Calculate average training loss\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validation phase\n",
        "    simp_model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_embeddings, batch_labels in val_loader:\n",
        "            outputs = simp_model(batch_embeddings)\n",
        "            loss = lossf(outputs, batch_labels)\n",
        "            val_loss += loss.item()\n",
        "    \n",
        "    # Calculate average validation loss\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Test phase\n",
        "    simp_model.eval()\n",
        "    test_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_embeddings, batch_labels in test_loader:\n",
        "            outputs = simp_model(batch_embeddings)\n",
        "            loss = lossf(outputs, batch_labels)\n",
        "            test_loss += loss.item()\n",
        "    \n",
        "    # Calculate average test loss\n",
        "    test_loss /= len(test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5536892736892737 0.5480107194392909\n",
            "Accuracy of the model on the data: 62.87%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the testset\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "simp_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_embeddings, batch_labels in test_loader:\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_labels.size(0)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "    f1 = f1_score(predicted , batch_labels, average='weighted')\n",
        "    f1_macro = f1_score(predicted, batch_labels, average='macro')\n",
        "    print (f1, f1_macro)\n",
        "    print(f'Accuracy of the model on the data: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6059259259259259 0.5000566893424037\n",
            "Accuracy of the model on the data: 63.04%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "simp_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_embeddings, batch_labels in val_loader:\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_labels.size(0)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "    f1 = f1_score(predicted , batch_labels, average='weighted')\n",
        "    f1_macro = f1_score(predicted, batch_labels, average='macro')\n",
        "    print (f1, f1_macro)\n",
        "    print(f'Accuracy of the model on the data: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score for class 0: 0.5110\n",
            "F1 score for class 1: 0.6118\n",
            "F1 score for class 2: 0.7702\n",
            "F1 score for class 3: 0.5034\n",
            "F1 score for class 4: 0.8580\n",
            "F1 score for class 5: 0.6841\n",
            "F1 score for class 6: 0.4709\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "simp_model.eval()\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_embeddings, batch_labels in test_loader:\n",
        "        outputs = simp_model(batch_embeddings)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predicted_labels.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_scores = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Print F1 score for each class\n",
        "for label, f1 in enumerate(f1_scores):\n",
        "    print(f\"F1 score for class {label}: {f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
